{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neuralnetworkskaran.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhBRzFU3z4SI",
        "colab_type": "code",
        "outputId": "aec76aa0-5b0e-4412-ed7b-ff101f3c047a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuXMB-nL0M1p",
        "colab_type": "code",
        "outputId": "0114e92d-ee3c-4723-dd19-08e73f1339b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd 'drive/My Drive/bep/AES-master'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/bep/AES-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoAzwfMF0Qlc",
        "colab_type": "code",
        "outputId": "9e18e926-3026-477d-c4ab-d45e169b16d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 300features_40minwords_10context\n",
            " Classification_Methods_Final.ipynb\n",
            "'Copy of Data_Cleaning_and_Transformation_Word2Vec_final.ipynb'\n",
            " Data_Cleaning_and_Transformation_Word2Vec_final.ipynb\n",
            " Data_Exploration_Final.ipynb\n",
            " dataloading1.py\n",
            " dataloading1.pyc\n",
            " essay2vec.py\n",
            " essay2vec.pyc\n",
            " essay2vec.py.gdoc\n",
            " \u001b[0m\u001b[01;34mlogs\u001b[0m/\n",
            " Neural_Network_Write_Up_final.ipynb\n",
            " \u001b[01;34m__pycache__\u001b[0m/\n",
            " README.md\n",
            " requiredimports.py\n",
            " requiredimports.pyc\n",
            " savetest.txt\n",
            " training_set_rel3.xlsx\n",
            " wordembedding.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov7vgGUQ1DNW",
        "colab_type": "code",
        "outputId": "b6e30e82-1281-4094-b4c1-40dbd43d37e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "pip install runipy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: runipy in /usr/local/lib/python2.7/dist-packages (0.1.5)\n",
            "Requirement already satisfied: ipykernel>=4.0.0 in /usr/local/lib/python2.7/dist-packages (from runipy) (4.10.1)\n",
            "Requirement already satisfied: Pygments>=1.6 in /usr/local/lib/python2.7/dist-packages (from runipy) (2.1.3)\n",
            "Requirement already satisfied: pyzmq>=14.1.0 in /usr/local/lib/python2.7/dist-packages (from runipy) (19.0.0)\n",
            "Requirement already satisfied: Jinja2>=2.7.2 in /usr/local/lib/python2.7/dist-packages (from runipy) (2.10.1)\n",
            "Requirement already satisfied: nbformat>=4.0.0 in /usr/local/lib/python2.7/dist-packages (from runipy) (4.4.0)\n",
            "Requirement already satisfied: nbconvert>=4.0.0 in /usr/local/lib/python2.7/dist-packages (from runipy) (5.5.0)\n",
            "Requirement already satisfied: ipython>=2.3.1 in /usr/local/lib/python2.7/dist-packages (from runipy) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python2.7/dist-packages (from ipykernel>=4.0.0->runipy) (4.3.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python2.7/dist-packages (from ipykernel>=4.0.0->runipy) (5.3.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python2.7/dist-packages (from ipykernel>=4.0.0->runipy) (4.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python2.7/dist-packages (from Jinja2>=2.7.2->runipy) (1.1.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python2.7/dist-packages (from nbformat>=4.0.0->runipy) (4.5.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python2.7/dist-packages (from nbformat>=4.0.0->runipy) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python2.7/dist-packages (from nbformat>=4.0.0->runipy) (0.2.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python2.7/dist-packages (from nbconvert>=4.0.0->runipy) (0.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python2.7/dist-packages (from nbconvert>=4.0.0->runipy) (0.6.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python2.7/dist-packages (from nbconvert>=4.0.0->runipy) (0.4.2)\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python2.7/dist-packages (from nbconvert>=4.0.0->runipy) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python2.7/dist-packages (from nbconvert>=4.0.0->runipy) (3.1.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from nbconvert>=4.0.0->runipy) (1.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python2.7/dist-packages (from ipython>=2.3.1->runipy) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python2.7/dist-packages (from ipython>=2.3.1->runipy) (0.7.5)\n",
            "Requirement already satisfied: backports.shutil-get-terminal-size; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from ipython>=2.3.1->runipy) (1.0.0)\n",
            "Requirement already satisfied: pathlib2; python_version == \"2.7\" or python_version == \"3.3\" in /usr/local/lib/python2.7/dist-packages (from ipython>=2.3.1->runipy) (2.3.4)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python2.7/dist-packages (from ipython>=2.3.1->runipy) (4.7.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python2.7/dist-packages (from ipython>=2.3.1->runipy) (4.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python2.7/dist-packages (from ipython>=2.3.1->runipy) (1.0.16)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python2.7/dist-packages (from ipython>=2.3.1->runipy) (44.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from traitlets>=4.1.0->ipykernel>=4.0.0->runipy) (1.12.0)\n",
            "Requirement already satisfied: enum34; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from traitlets>=4.1.0->ipykernel>=4.0.0->runipy) (1.1.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python2.7/dist-packages (from jupyter-client->ipykernel>=4.0.0->runipy) (2.5.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python2.7/dist-packages (from tornado>=4.0->ipykernel>=4.0.0->runipy) (2019.6.16)\n",
            "Requirement already satisfied: singledispatch in /usr/local/lib/python2.7/dist-packages (from tornado>=4.0->ipykernel>=4.0.0->runipy) (3.4.0.3)\n",
            "Requirement already satisfied: backports-abc>=0.4 in /usr/local/lib/python2.7/dist-packages (from tornado>=4.0->ipykernel>=4.0.0->runipy) (0.5)\n",
            "Requirement already satisfied: functools32; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.0.0->runipy) (3.2.3.post2)\n",
            "Requirement already satisfied: configparser>=3.5; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from entrypoints>=0.2.2->nbconvert>=4.0.0->runipy) (3.7.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python2.7/dist-packages (from bleach->nbconvert>=4.0.0->runipy) (0.5.1)\n",
            "Requirement already satisfied: scandir; python_version < \"3.5\" in /usr/local/lib/python2.7/dist-packages (from pathlib2; python_version == \"2.7\" or python_version == \"3.3\"->ipython>=2.3.1->runipy) (1.10.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python2.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=2.3.1->runipy) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python2.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=2.3.1->runipy) (0.1.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1DkR6Sk7e-J",
        "colab_type": "code",
        "outputId": "98d722e6-b61a-4e3f-bbf3-d543cef7b076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li4wP7RUFhW5",
        "colab_type": "code",
        "outputId": "8764f7e3-39ea-4903-b985-e3411821b927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        }
      },
      "source": [
        "pip install gensim==0.13.4.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim==0.13.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/45/7ee51ecf36bae1fa27f6de4d392d5773d0184c0acf0f6780ebf167cbcd4a/gensim-0.13.4.1.tar.gz (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.3 in /usr/local/lib/python2.7/dist-packages (from gensim==0.13.4.1) (1.16.4)\n",
            "Requirement already satisfied: scipy>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from gensim==0.13.4.1) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python2.7/dist-packages (from gensim==0.13.4.1) (1.12.0)\n",
            "Requirement already satisfied: smart_open>=1.2.1 in /usr/local/lib/python2.7/dist-packages (from gensim==0.13.4.1) (1.8.4)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python2.7/dist-packages (from smart_open>=1.2.1->gensim==0.13.4.1) (0.98)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python2.7/dist-packages (from smart_open>=1.2.1->gensim==0.13.4.1) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python2.7/dist-packages (from smart_open>=1.2.1->gensim==0.13.4.1) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python2.7/dist-packages (from smart_open>=1.2.1->gensim==0.13.4.1) (1.9.189)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests->smart_open>=1.2.1->gensim==0.13.4.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests->smart_open>=1.2.1->gensim==0.13.4.1) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests->smart_open>=1.2.1->gensim==0.13.4.1) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests->smart_open>=1.2.1->gensim==0.13.4.1) (2.8)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python2.7/dist-packages (from boto3->smart_open>=1.2.1->gensim==0.13.4.1) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.189 in /usr/local/lib/python2.7/dist-packages (from boto3->smart_open>=1.2.1->gensim==0.13.4.1) (1.12.189)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from boto3->smart_open>=1.2.1->gensim==0.13.4.1) (0.2.1)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python2.7/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->smart_open>=1.2.1->gensim==0.13.4.1) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python2.7/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->smart_open>=1.2.1->gensim==0.13.4.1) (2.5.3)\n",
            "Requirement already satisfied: futures<4.0.0,>=2.2.0; python_version == \"2.6\" or python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from s3transfer<0.3.0,>=0.2.0->boto3->smart_open>=1.2.1->gensim==0.13.4.1) (3.2.0)\n",
            "Building wheels for collected packages: gensim\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gensim: filename=gensim-0.13.4.1-cp27-cp27mu-linux_x86_64.whl size=4815637 sha256=9dac032ba97bcaee2dc33f795dca1589e30d1af4a5c27dcdaa0acdad43c4388f\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/4a/36/34fbd4f1e3a18fe5a2bb3d4366a44711ad0c34adc8251829d9\n",
            "Successfully built gensim\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-0.13.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF_95BluBXcO",
        "colab_type": "code",
        "outputId": "8d79e520-f86c-4a9a-efbd-993618d6b3e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pip install scikit-learn==0.18"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn==0.18 in /usr/local/lib/python2.7/dist-packages (0.18)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caX0gXxCAgFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python requiredimports.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wlj1YUzNy7bu",
        "colab_type": "code",
        "outputId": "8dc78289-c8ab-4bb2-fd8d-fcfe81c6077a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "pip install paramiko"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting paramiko\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/1e/1e08baaaf6c3d3df1459fd85f0e7d2d6aa916f33958f151ee1ecc9800971/paramiko-2.7.1-py2.py3-none-any.whl (206kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 3.2MB/s \n",
            "\u001b[?25hCollecting pynacl>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/25/e605574f24948a8a53b497744e93f061eb1dbe7c44b6465fc1c172d591aa/PyNaCl-1.3.0-cp27-cp27mu-manylinux1_x86_64.whl (762kB)\n",
            "\u001b[K     |████████████████████████████████| 768kB 44.8MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/36/9a0227d048e98409f012570f7bef8a8c2373b9c9c5dfbf82963cbae05ede/bcrypt-3.1.7-cp27-cp27mu-manylinux1_x86_64.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.3MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/f9/ee6878ab822eef403a4282c8ce80d56e3121c9576a6544377df809363b50/cryptography-2.9.2-cp27-cp27mu-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 39.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from pynacl>=1.0.1->paramiko) (1.12.0)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from pynacl>=1.0.1->paramiko) (1.12.3)\n",
            "Requirement already satisfied: enum34; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from cryptography>=2.5->paramiko) (1.1.6)\n",
            "Requirement already satisfied: ipaddress; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from cryptography>=2.5->paramiko) (1.0.22)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python2.7/dist-packages (from cffi>=1.4.1->pynacl>=1.0.1->paramiko) (2.19)\n",
            "Installing collected packages: pynacl, bcrypt, cryptography, paramiko\n",
            "Successfully installed bcrypt-3.1.7 cryptography-2.9.2 paramiko-2.7.1 pynacl-1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJDtvgh7gYVC",
        "colab_type": "code",
        "outputId": "5572b950-f711-442c-f052-823877b1840b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        }
      },
      "source": [
        "pip install tensorflow==1.4.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/72/a420e22dc93416d30981e87a2318823ec09a9b18631369df0e7d9d164073/tensorflow-1.4.0-cp27-cp27mu-manylinux1_x86_64.whl (40.7MB)\n",
            "\u001b[K     |████████████████████████████████| 40.8MB 98kB/s \n",
            "\u001b[?25hCollecting tensorflow-tensorboard<0.5.0,>=0.4.0rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/cd/f3d14d441eb1c5228aaf7e12e8e94895ae73e9af50383e481610b34357bd/tensorflow_tensorboard-0.4.0-py2-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 31.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.4.0) (2.0.0)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.4.0) (1.1.6)\n",
            "Requirement already satisfied: protobuf>=3.3.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.4.0) (3.8.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.4.0) (0.34.2)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.4.0) (1.0.post1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.4.0) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.4.0) (1.16.4)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: futures>=3.1.1; python_version < \"3.2\" in /usr/local/lib/python2.7/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0) (3.2.0)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 44.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0) (3.1.1)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.4.0) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.4.0) (5.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.3.0->tensorflow==1.4.0) (44.1.0)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp27-none-any.whl size=107221 sha256=92c3fe4b14900ddf338153a906097d2631436e45a6002d2edd343fabe11a141d\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 has requirement scikit-learn>=0.19.1, but you'll have scikit-learn 0.18 which is incompatible.\u001b[0m\n",
            "Installing collected packages: html5lib, bleach, tensorflow-tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.0\n",
            "    Uninstalling bleach-3.1.0:\n",
            "      Successfully uninstalled bleach-3.1.0\n",
            "  Found existing installation: tensorflow 2.1.0\n",
            "    Uninstalling tensorflow-2.1.0:\n",
            "      Successfully uninstalled tensorflow-2.1.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorflow-1.4.0 tensorflow-tensorboard-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqbD-WivcGf4",
        "colab_type": "code",
        "outputId": "d74ce4de-4e17-488c-d0c9-d69796abaf7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Code referred from: https://github.com/NaruBeast/Automated-Essay-Grader\n",
        "#Updated parameters and deprecated functions\n",
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from scipy.stats import spearmanr\n",
        "import nltk.data\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "from gensim.models import word2vec\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import timeit\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "xl_workbook = pd.ExcelFile('training_set_rel3.xlsx')\n",
        "df_all = xl_workbook.parse(\"training_set\")\n",
        "df_all = df_all[df_all['domain1_score']<61]\n",
        "df_all = df_all.dropna(axis = 1)\n",
        "df_all = df_all.drop('rater1_domain1', 1)\n",
        "df_all = df_all.drop('rater2_domain1', 1)\n",
        "df_all = df_all.drop('essay_id', 1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_all['essay'], df_all['domain1_score'], test_size=0.10)\n",
        "#print(y_train)\n",
        "#print(len(y_train))\n",
        "y_train = y_train.values.reshape((len(y_train),1))\n",
        "#print(y_train)\n",
        "y_test = y_test.values.reshape((len(y_test), 1))\n",
        "#print(y_test)\n",
        "\n",
        "def essay_to_wordlist(essay_v, remove_stopwords):\n",
        "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
        "    words = essay_v.lower().split()\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    return (words)\n",
        "\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "def essay_to_sentences(essay_v, remove_stopwords):\n",
        "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
        "    return sentences\n",
        "\n",
        "sentences = []\n",
        "\n",
        "for essay_v in X_train:    \n",
        "    sentences += essay_to_sentences(essay_v, remove_stopwords = True)\n",
        "\n",
        "for essay_v in X_test:\n",
        "    sentences += essay_to_sentences(essay_v, remove_stopwords = True)\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "num_features = 300 \n",
        "min_word_count = 40\n",
        "num_workers = 4\n",
        "context = 10\n",
        "downsampling = 1e-3\n",
        "\n",
        "print (\"Training model...\")\n",
        "model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
        "\n",
        "model.init_sims(replace=True)\n",
        "\n",
        "model_name = \"300features_40minwords_10context\"\n",
        "model.save(model_name)\n",
        "\n",
        "def makeFeatureVec(words, model, num_features):\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    nwords = 0.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    for word in words:\n",
        "        if word in index2word_set: \n",
        "            nwords = nwords + 1.\n",
        "            featureVec = np.add(featureVec,model[word])        \n",
        "    featureVec = np.divide(featureVec,nwords)\n",
        "    return featureVec\n",
        "\n",
        "def getAvgFeatureVecs(essays, model, num_features):\n",
        "    counter = 0\n",
        "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
        "    for essay in essays:\n",
        "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
        "        counter = counter + 1\n",
        "    return essayFeatureVecs\n",
        "\n",
        "print (\"Creating average feature vecs for Training Essays\")\n",
        "clean_train_essays = []\n",
        "for essay_v in X_train:\n",
        "    clean_train_essays.append( essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
        "trainDataVecs = getAvgFeatureVecs( clean_train_essays, model, num_features )\n",
        "\n",
        "\n",
        "clean_test_essays = []\n",
        "for essay_v in X_test:\n",
        "    clean_test_essays.append( essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
        "testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-29 00:41:22,160 : INFO : collecting all words and their counts\n",
            "2020-04-29 00:41:22,162 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2020-04-29 00:41:22,191 : INFO : PROGRESS: at sentence #10000, processed 82641 words, keeping 8184 word types\n",
            "2020-04-29 00:41:22,218 : INFO : PROGRESS: at sentence #20000, processed 166976 words, keeping 11766 word types\n",
            "2020-04-29 00:41:22,245 : INFO : PROGRESS: at sentence #30000, processed 252601 words, keeping 14741 word types\n",
            "2020-04-29 00:41:22,272 : INFO : PROGRESS: at sentence #40000, processed 335600 words, keeping 17110 word types\n",
            "2020-04-29 00:41:22,300 : INFO : PROGRESS: at sentence #50000, processed 419341 words, keeping 19304 word types\n",
            "2020-04-29 00:41:22,327 : INFO : PROGRESS: at sentence #60000, processed 503362 words, keeping 21344 word types\n",
            "2020-04-29 00:41:22,355 : INFO : PROGRESS: at sentence #70000, processed 587804 words, keeping 23241 word types\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-04-29 00:41:22,389 : INFO : PROGRESS: at sentence #80000, processed 671500 words, keeping 24915 word types\n",
            "2020-04-29 00:41:22,417 : INFO : PROGRESS: at sentence #90000, processed 756322 words, keeping 26762 word types\n",
            "2020-04-29 00:41:22,444 : INFO : PROGRESS: at sentence #100000, processed 841266 words, keeping 28372 word types\n",
            "2020-04-29 00:41:22,478 : INFO : PROGRESS: at sentence #110000, processed 927140 words, keeping 30058 word types\n",
            "2020-04-29 00:41:22,512 : INFO : PROGRESS: at sentence #120000, processed 1010691 words, keeping 31505 word types\n",
            "2020-04-29 00:41:22,539 : INFO : PROGRESS: at sentence #130000, processed 1094750 words, keeping 32963 word types\n",
            "2020-04-29 00:41:22,567 : INFO : PROGRESS: at sentence #140000, processed 1179885 words, keeping 34462 word types\n",
            "2020-04-29 00:41:22,596 : INFO : PROGRESS: at sentence #150000, processed 1265757 words, keeping 35818 word types\n",
            "2020-04-29 00:41:22,624 : INFO : PROGRESS: at sentence #160000, processed 1349035 words, keeping 37154 word types\n",
            "2020-04-29 00:41:22,641 : INFO : collected 37854 word types from a corpus of 1393161 raw words and 165002 sentences\n",
            "2020-04-29 00:41:22,643 : INFO : Loading a fresh vocabulary\n",
            "2020-04-29 00:41:22,670 : INFO : min_count=40 retains 2902 unique words (7% of original 37854, drops 34952)\n",
            "2020-04-29 00:41:22,672 : INFO : min_count=40 leaves 1266676 word corpus (90% of original 1393161, drops 126485)\n",
            "2020-04-29 00:41:22,691 : INFO : deleting the raw counts dictionary of 37854 items\n",
            "2020-04-29 00:41:22,694 : INFO : sample=0.001 downsamples 66 most-common words\n",
            "2020-04-29 00:41:22,696 : INFO : downsampling leaves estimated 1116198 word corpus (88.1% of prior 1266676)\n",
            "2020-04-29 00:41:22,697 : INFO : estimated required memory for 2902 words and 300 dimensions: 8415800 bytes\n",
            "2020-04-29 00:41:22,708 : INFO : resetting layer weights\n",
            "2020-04-29 00:41:22,749 : INFO : training model with 4 workers on 2902 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
            "2020-04-29 00:41:22,752 : INFO : expecting 165002 sentences, matching count from corpus used for vocabulary survey\n",
            "2020-04-29 00:41:23,760 : INFO : PROGRESS: at 5.35% examples, 296085 words/s, in_qsize 8, out_qsize 0\n",
            "2020-04-29 00:41:24,768 : INFO : PROGRESS: at 11.25% examples, 310881 words/s, in_qsize 8, out_qsize 0\n",
            "2020-04-29 00:41:25,799 : INFO : PROGRESS: at 16.96% examples, 313042 words/s, in_qsize 7, out_qsize 0\n",
            "2020-04-29 00:41:26,788 : INFO : PROGRESS: at 22.55% examples, 311958 words/s, in_qsize 8, out_qsize 0\n",
            "2020-04-29 00:41:27,789 : INFO : PROGRESS: at 28.31% examples, 313521 words/s, in_qsize 8, out_qsize 0\n",
            "2020-04-29 00:41:28,804 : INFO : PROGRESS: at 33.88% examples, 312625 words/s, in_qsize 7, out_qsize 0\n",
            "2020-04-29 00:41:29,860 : INFO : PROGRESS: at 39.76% examples, 313079 words/s, in_qsize 8, out_qsize 0\n",
            "2020-04-29 00:41:30,859 : INFO : PROGRESS: at 45.67% examples, 314358 words/s, in_qsize 8, out_qsize 0\n",
            "2020-04-29 00:41:31,891 : INFO : PROGRESS: at 51.42% examples, 313910 words/s, in_qsize 8, out_qsize 0\n",
            "2020-04-29 00:41:32,920 : INFO : PROGRESS: at 57.28% examples, 314403 words/s, in_qsize 8, out_qsize 0\n",
            "2020-04-29 00:41:33,932 : INFO : PROGRESS: at 63.14% examples, 315255 words/s, in_qsize 7, out_qsize 0\n",
            "2020-04-29 00:41:34,978 : INFO : PROGRESS: at 69.07% examples, 315152 words/s, in_qsize 8, out_qsize 0\n",
            "2020-04-29 00:41:36,013 : INFO : PROGRESS: at 74.93% examples, 315339 words/s, in_qsize 8, out_qsize 0\n",
            "2020-04-29 00:41:37,015 : INFO : PROGRESS: at 80.79% examples, 316148 words/s, in_qsize 8, out_qsize 0\n",
            "2020-04-29 00:41:38,021 : INFO : PROGRESS: at 86.42% examples, 315807 words/s, in_qsize 8, out_qsize 0\n",
            "2020-04-29 00:41:39,034 : INFO : PROGRESS: at 92.01% examples, 315326 words/s, in_qsize 8, out_qsize 0\n",
            "2020-04-29 00:41:40,048 : INFO : PROGRESS: at 97.73% examples, 315355 words/s, in_qsize 8, out_qsize 1\n",
            "2020-04-29 00:41:40,344 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-04-29 00:41:40,357 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-04-29 00:41:40,364 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-04-29 00:41:40,384 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-04-29 00:41:40,386 : INFO : training on 6965805 raw words (5579842 effective words) took 17.6s, 316505 effective words/s\n",
            "2020-04-29 00:41:40,388 : INFO : precomputing L2-norms of word weight vectors\n",
            "2020-04-29 00:41:40,416 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
            "2020-04-29 00:41:40,419 : INFO : not storing attribute syn0norm\n",
            "2020-04-29 00:41:40,420 : INFO : not storing attribute cum_table\n",
            "2020-04-29 00:41:40,470 : INFO : saved 300features_40minwords_10context\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Creating average feature vecs for Training Essays\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fV3Qe8MUQJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code referred from: https://github.com/NaruBeast/Automated-Essay-Grader\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.zeros(shape)\n",
        "    return tf.Variable(initial)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9jZVTcMQlTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code referred from: https://github.com/NaruBeast/Automated-Essay-Grader\n",
        "\n",
        "batch_size = 300\n",
        "hidden_nodes_1 = 500\n",
        "hidden_nodes_2 = 750\n",
        "size = testDataVecs.shape[1]\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "    # Input data.\n",
        "    with tf.name_scope(\"Input\"):\n",
        "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, size))\n",
        "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 1))\n",
        "    \n",
        "    tf_test_dataset = tf.constant(testDataVecs)\n",
        "      \n",
        "    layer1_weights = weight_variable([size, hidden_nodes_1])\n",
        "    layer1_biases = bias_variable([hidden_nodes_1]) \n",
        "    \n",
        "    layer2_weights = weight_variable([hidden_nodes_1, hidden_nodes_2])\n",
        "    layer2_biases = bias_variable([hidden_nodes_2])\n",
        "    \n",
        "    layer3_weights = weight_variable([hidden_nodes_2, 1])\n",
        "    layer3_biases = bias_variable([1])\n",
        "    \n",
        "    def model(data):\n",
        "        with tf.name_scope(\"Layer_1\"):\n",
        "            layer1 = tf.nn.relu(tf.matmul(data, layer1_weights) + layer1_biases)\n",
        "        \n",
        "        with tf.name_scope(\"Layer_2\"):\n",
        "            \n",
        "            layer2 = tf.nn.relu(tf.matmul(layer1, layer2_weights) + layer2_biases)\n",
        "\n",
        "        with tf.name_scope(\"Layer_3\"):\n",
        "            layer3 = tf.nn.relu(tf.matmul(layer2, layer3_weights) + layer3_biases)\n",
        "        return layer3\n",
        "    \n",
        "    # Training computation.\n",
        "    yhat = model(tf_train_dataset)\n",
        "    \n",
        "    with tf.name_scope(\"Loss\"):\n",
        "        loss = tf.reduce_mean(tf.square(yhat - tf_train_labels))\n",
        "    \n",
        "    # Optimizer.\n",
        "    # learning rate decay\n",
        "    global_step = tf.Variable(0)  # count  number of steps taken.\n",
        "    start_learning_rate = 0.001\n",
        "    learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 100000, 0.96, staircase=True)\n",
        "    \n",
        "    with tf.name_scope(\"Train\"):\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "    \n",
        "    tf.summary.scalar(\"loss\", loss) #tf.scalar_summary(\"loss\", loss)\n",
        "    \n",
        "    # Predictions for the training, validation, and test data.\n",
        "    train_prediction = yhat\n",
        "    test_prediction = model(tf_test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYR338eAQlTr",
        "colab_type": "code",
        "outputId": "8da5be7f-8bb5-4441-c019-38df28ff92d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "#Code referred from: https://github.com/NaruBeast/Automated-Essay-Grader\n",
        "\n",
        "test_preds = pd.DataFrame()\n",
        "# Re-define the function to include the keep probability\n",
        "l_array = []\n",
        "start = timeit.timeit()\n",
        "num_epochs = 3001\n",
        "def run_session(num_epochs, name):\n",
        "    with tf.Session(graph=graph) as session:\n",
        "        writer = tf.summary.FileWriter(\"logs/\", session.graph) #writer = tf.train.SummaryWriter(\"logs/\", session.graph)\n",
        "        tf.initialize_all_variables().run()\n",
        "        print(\"Initialized\")\n",
        "        for epoch in range(num_epochs):\n",
        "            offset = (epoch * batch_size) % (y_train.shape[0] - batch_size)\n",
        "            batch_data = trainDataVecs[offset:(offset + batch_size), :]\n",
        "            batch_labels = y_train[offset:(offset + batch_size)]\n",
        "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "            l_array.append(l)\n",
        "            if (epoch % 500 == 0):\n",
        "                print(\"Minibatch Loss at Epoch {}: {:.3f}\".format(epoch, l))\n",
        "                rho, pval = (spearmanr(predictions, batch_labels))\n",
        "                print(\"Minibatch Spearman Score: {:.4f}\".format(rho))\n",
        "        final_rho, pval = spearmanr(test_prediction.eval(), y_test)\n",
        "        print(\"Test Spearman Score: {:.4f}\".format(final_rho))\n",
        "        test_preds[name] = test_prediction.eval().ravel()\n",
        "        filesave = np.zeros((len(y_test), 2))\n",
        "        to_use = test_prediction.eval()\n",
        "        \n",
        "        for ii in range(len(y_test)):           \n",
        "            filesave[ii, 0] = y_test[ii]\n",
        "            filesave[ii, 1] = to_use[ii]\n",
        "        \n",
        "        np.savetxt('savetest.txt', filesave, delimiter=\",\", fmt=\"%d\") \n",
        "        \n",
        "run_session(num_epochs, \"Deep_NN\")\n",
        "total = timeit.timeit() - start\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch Loss at Epoch 0: 120.671\n",
            "Minibatch Spearman Score: 0.0728\n",
            "Minibatch Loss at Epoch 500: 4.720\n",
            "Minibatch Spearman Score: 0.9060\n",
            "Minibatch Loss at Epoch 1000: 3.189\n",
            "Minibatch Spearman Score: 0.9303\n",
            "Minibatch Loss at Epoch 1500: 1.839\n",
            "Minibatch Spearman Score: 0.9332\n",
            "Minibatch Loss at Epoch 2000: 2.706\n",
            "Minibatch Spearman Score: 0.9589\n",
            "Minibatch Loss at Epoch 2500: 1.328\n",
            "Minibatch Spearman Score: 0.9602\n",
            "Minibatch Loss at Epoch 3000: 1.105\n",
            "Minibatch Spearman Score: 0.9530\n",
            "Test Spearman Score: 0.9395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2WclnzyMHZ5",
        "colab_type": "code",
        "outputId": "566975de-937b-48cd-f1e2-3075624687da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "#Code referred from: https://github.com/NaruBeast/Automated-Essay-Grader\n",
        "\n",
        "plt.plot(range(num_epochs), l_array, '.')\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.title(\"Cost vs Epoch\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAIABJREFUeJzt3XucXWV97/HPd++5JCGBDMkAIVdGeaGCVJMRhtLji0qrYKmhigpSiwrmtNXTWu1RvBxQTntqL1brq1QLaMUWAnIr1KNHLo231gnMRC4JMSWMTJgQcp1cSMLM7Nm/88dee7Iz2bPnQvbsPdnf9+s1zFrPWnvtZ7En853nWc96liICMzOz4VKVroCZmVUnB4SZmRXlgDAzs6IcEGZmVpQDwszMinJAmJlZUQ4IsxogKSS9utL1sKnFAWFTkqT3SeqQ9JKkLZK+L+nXXuExn5P0G0erjqO8z8Gk7vmvvy/3+5qNV12lK2A2XpI+DlwL/D7wA6AfuAhYDvy0glUbj9+OiIcrXQmzUtyCsClF0gnADcBHIuLeiNgfEQMR8W8R8T+TfRolfUXSC8nXVyQ1JtvmSvqupN2Sdkn6iaSUpH8GFgH/lvxF/8ki771e0iUF63WStktaKmmapH+RtDM59mOSTp7A+X1A0n9I+ntJeyT9QtKFBdtPlfRAUveNkj5csC0t6TOSnpW0T1KnpIUFh/8NSc8k9btRksZbP6stDgibas4DpgH3ldjns0Ab8AbgV4BzgM8l2z4B9ADNwMnAZ4CIiPcDm8j9ZT8zIv6qyHFXAlcUrL8N2BERa4CrgBOAhcAccq2bgxM5QeBc4FlgLnA9cK+kE5NtdyT1PxW4DPg/kt6SbPt4Ur+3A8cDHwIOFBz3EuBNwNnAe5L6m43IAWFTzRxyv5QzJfa5ErghIrZFxHbgC8D7k20DwDxgcdLy+EmMfUKy24F3SJqRrL+PXGjkjzsHeHVEDEZEZ0TsLXGsf03+ks9/fbhg2zbgK0n97gQ2AL+VtAbOBz4VES9HxOPALcDvJa+7BvhcRGyInCciYmfBcb8YEbsjYhOwilyAmo3IAWFTzU5grqRS189OBboL1ruTMoC/BjYCD0rqknTtWN84IjYC64HfTkLiHeRCA+CfyV0PuSPp1vorSfUlDndpRMwu+Lq5YNvmYaGVr/+pwK6I2Dds2/xkeSG5lsdIXixYPgDMLLGvmQPCppyfAX3ApSX2eQFYXLC+KCkjIvZFxCciooXcL/iPF/Txj6Ulke9mWg48nYQGyV/7X4iI1wG/Sq475/dGPkxJ84ddH8jX/wXgREmzhm3bnCw/D7xqgu9pdgQHhE0pEbEHuA64UdKlkmZIqpd0saT8dYOVwOckNUuam+z/LwCSLpH06uQX8B5gEMgmr9sKtIxShTuAtwJ/wKHWA5J+XdLrJaWBveS6nLLFDzGqk4A/Ss7r3cBrge9FxPPAfwJ/kVwUPxu4On9u5Lqb/rek05VztqQ5E6yDmQPCpp6I+BK5C7KfA7aT+8v5o8C/Jrv8GdABPAk8BaxJygBOBx4GXiLXGvmHiFiVbPsLcsGyW9KfjvDeW5LX/SpwZ8GmU4C7yYXDeuBH5LqdRpIfLZX/Krzovjqp5w7gz4HLCq4lXAEsIdeauA+4vmC47N8C3wEeTOrxDWB6iTqYlSQ/MMisekj6AHBNRLyim/7Mjga3IMzMrCgHhJmZFeUuJjMzK8otCDMzK2pKT9Y3d+7cWLJkSaWrYWY2pXR2du6IiObR9pvSAbFkyRI6OjoqXQ0zsylFUvfoe7mLyczMRuCAMDOzohwQZmZWlAPCzMyKckCYmVlRDggzMyuqZgOis7uXG1dtpLO7t9JVMTOrSlP6PoiJ6uzu5cpb2unPZGmoS3HbNW0sW9xU6WqZmVWVmmxBtHftpD+TJRswkMnS3rVz9BeZmdWYmgyItpY5NNSlSAvq61K0tfihW2Zmw9VkF9OyxU3cdk0b7V07aWuZ4+4lM7MiajIgIBcSDgYzs5HVZBeTmZmNzgFhZmZFOSDMzKwoB4SZmRXlgDAzs6IcEGZmVpQDwszMiipbQEj6pqRtktYW2fYJSSFpbrIuSV+VtFHSk5KWlqteZmY2NuVsQXwLuGh4oaSFwFuBTQXFFwOnJ18rgK+VsV6AZ3M1MxtN2e6kjogfS1pSZNOXgU8C9xeULQe+HREBtEuaLWleRGwpR908m6uZ2egm9RqEpOXA5oh4Ytim+cDzBes9SVmxY6yQ1CGpY/v27ROqh2dzNTMb3aQFhKQZwGeA617JcSLipohojYjW5ubmCR3Ds7mamY1uMifrexVwGvCEJIAFwBpJ5wCbgYUF+y5IysrCs7mamY1u0gIiIp4CTsqvS3oOaI2IHZIeAD4q6Q7gXGBPua4/5Hk2VzOz0so5zHUl8DPgDEk9kq4usfv3gC5gI3Az8IflqpeZmY1NOUcxXTHK9iUFywF8pFx1MTOz8fOd1GZmVpQDwszMinJAmJlZUQ4IMzMrygFhZmZFOSDMzKwoB4SZmRXlgDAzs6IcEGZmVpQDwszMinJAmJlZUQ4IMzMrygFhZmZFOSDMzKwoB4SZmRXlgDAzs6IcEGZmVlQ5Hzn6TUnbJK0tKPtrSb+Q9KSk+yTNLtj2aUkbJW2Q9LZy1Qugs7uXG1dtpLO7t5xvY2Y2pZWzBfEt4KJhZQ8BZ0XE2cB/AZ8GkPQ64HLgzOQ1/yApXY5KdXb3cuUt7XzpwQ1ceUu7Q8LMbARlC4iI+DGwa1jZgxGRSVbbgQXJ8nLgjojoi4hfAhuBc8pRr/aunfRnsmQDBjJZ2rt2luNtzMymvEpeg/gQ8P1keT7wfMG2nqTsCJJWSOqQ1LF9+/Zxv2lbyxwa6lKkBfV1Kdpa5oz7GGZmtaCuEm8q6bNABrhtvK+NiJuAmwBaW1tjvK9ftriJ265po71rJ20tc1i2uGm8hzAzqwmTHhCSPgBcAlwYEflf8JuBhQW7LUjKymLZ4iYHg5nZKCa1i0nSRcAngXdExIGCTQ8Al0tqlHQacDrw6GTWzczMDle2FoSklcAFwFxJPcD15EYtNQIPSQJoj4jfj4h1kr4DPE2u6+kjETFYrrqZmdnodKiXZ+ppbW2Njo6OSlfDzGxKkdQZEa2j7ec7qc3MrCgHhJmZFeWAMDOzohwQZmZWVM0GhCfsMzMrrSJ3UldafsK+/kyWhroUt13T5hvnzMyGqckWhCfsMzMbXU0GhCfsMzMbXU12MXnCPjOz0dVkQIAn7DMzG01NdjGZmdnoHBBmZlaUA8LMzIpyQJiZWVEOCDMzK8oBYWZmRTkgzMysqJoNCE/WZ2ZWWtkCQtI3JW2TtLag7ERJD0l6JvnelJRL0lclbZT0pKSl5aoXHJqs70sPbuDKW9odEmZmRZSzBfEt4KJhZdcCj0TE6cAjyTrAxcDpydcK4GtlrJcn6zMzG4OyBURE/BjYNax4OXBrsnwrcGlB+bcjpx2YLWleuerW1jKHupQQkE7Jk/WZmRUx2dcgTo6ILcnyi8DJyfJ84PmC/XqSsiNIWiGpQ1LH9u3bJ14T6fDvZmZ2mIpdpI6IAGICr7spIlojorW5uXlC793etZPMYJYABgfdxWRmVsxkB8TWfNdR8n1bUr4ZWFiw34KkrCz8PAgzs9FN9nTfDwBXAV9Mvt9fUP5RSXcA5wJ7Crqijjo/D8LMbHRlCwhJK4ELgLmSeoDryQXDdyRdDXQD70l2/x7wdmAjcAD4YLnqlefnQZiZlVa2gIiIK0bYdGGRfQP4SLnqYmZm4+c7qX2TnJlZUTX5yNHO7l6uuLmdgUyW+roUKz/c5u4mM7NharIFce+aHvozuWGu/Zks967pqXSVzMyqTk0GxPCbL8Z9M4aZWQ2oyYB419IFNKRzU200pMW7li6odJXMzKpOTV6DWLa4iZUrzvN9EGZmJdRkQIDvgzAzG01NdjGBh7mamY2mJlsQHuZqZja6mmxBeJirmdnoajIgPMzVzGx0NRkQHuZqZja6mrwG4WGuZmajq8mAAA9zNTMbTU12MZmZ2egcEGZmVpQDwszMiqpIQEj6E0nrJK2VtFLSNEmnSVotaaOkOyU1VKJuZmaWM6aAkPTusZSN8VjzgT8CWiPiLCANXA78JfDliHg10AtcPZHjm5nZ0THWFsSnx1g2VnXAdEl1wAxgC/AW4O5k+63Apa/g+GZm9gqVHOYq6WLg7cB8SV8t2HQ8kJnIG0bEZkl/A2wCDgIPAp3A7ojIH7MHmD9CnVYAKwAWLVo0kSoAufmYfB+EmdnIRrsP4gWgA3gHuV/iefuAP5nIG0pqApYDpwG7gbuAi8b6+oi4CbgJoLW1dUKzZHR293LlLe30Z7I01KW47RpP1mdmNlzJgIiIJ4AnJN0eEQMw9At+YURMdJ7s3wB+GRHbk+PdC5wPzJZUl7QiFgCbJ3j8UbV37aQ/kyUbMJDJ0t610wFhZjbMWK9BPCTpeEknAmuAmyV9eYLvuQlokzRDkoALgaeBVcBlyT5XAfdP8PijamuZQ0NdirSgvi5FW8uccr2VmdmUNdapNk6IiL2SrgG+HRHXS3pyIm8YEasl3U0uaDLAz8l1Gf1f4A5Jf5aUfWMixx+LZYubuO2aNl+DMDMrYawBUSdpHvAe4LOv9E0j4nrg+mHFXcA5r/TYY+W5mMzMShtrF9MNwA+AZyPiMUktwDPlq5aZmVXamFoQEXEXudFG+fUu4F3lqpSZmVXeWO+kXiDpPknbkq97JPkpO2Zmx7CxdjH9E/AAcGry9W9J2ZTW2d3Ljas20tk90RG7ZmbHrrFepG6OiMJA+Jakj5WjQpPFN8uZmZU21hbETkm/KymdfP0usLOcFSu3YjfLmZnZIWMNiA+RG+L6IrmJ9S4DPlCmOk0K3yxnZlbaWLuYbgCuyk+vkdxR/TfkgmNK8s1yZmaljTUgzi6ceykidkl6Y5nqNGl8s5yZ2cjG2sWUSibpA4ZaEGMNFzMzm4LG+kv+S8DPJOVvlns38OflqZKZmVWDsd5J/W1JHeSe+gbwzoh4unzVMjOzShtzN1ESCA4FM7MaMdZrEGZmVmNqOiA81YaZ2chqdiSSp9owMyutZlsQnmrDzKy0mg0IT7VhZlZaRbqYJM0GbgHOAoLclB0bgDuBJcBzwHsK794+2jzVhplZaZVqQfwd8P8i4jXArwDrgWuBRyLidOCRZN3MzCpk0lsQkk4A3kwyG2xE9AP9kpYDFyS73Qr8EPhUuerR2d3LFTf9jIHBoD4tVq44z60IM7MClWhBnAZsB/5J0s8l3SLpOODkiNiS7PMicHKxF0taIalDUsf27dsnXIl71vTQPxgE0D8Y3LOmZ8LHMjM7FlUiIOqApcDXIuKNwH6GdSdFRJC7NnGEiLgpIlojorW5uXnCldAo62Zmta4SAdED9ETE6mT9bnKBsVXSPIDk+7ZyVmJWY13JdTOzWjfpARERLwLPSzojKbqQ3BxPDwBXJWVXAfeXsx7rtuwtuW5mVusq9Wfz/wBuk9QAdAEfJBdW35F0NdBN7hGnZXPxWfP4yTM7Dls3M7NDKhIQEfE40Fpk04WTVYf3nbsIgO+v3cLFZ80bWjczs5ya7ng/45RZ9B7o54xTZlW6KmZmVadmA8KT9ZmZlVazczF5sj4zs9JqNiA8WZ+ZWWk128XkyfrMzEqr2RaEmZmVVrMtCF+kNjMrrWZbEL5IbWZWWs0GhC9Sm5mVVrNdTL5IbWZWWs0GBDAUCvnuJYeEmdkhNR0Qt6/exHX3ryUb4QvVZmbD1Ow1iM7uXq67fy2ZbJAN6PeFajOzw9RsQLR37WQwe+ihdSnJF6rNzArUbEC0tcyhsT5FCkgL3vKakypdJTOzqlKzAZEfxXT5uYtIp1M8vH4rV97STmd3b6WrZmZWFWo2ICAXEjv29Q3dMNc/4OsQZmZ5FQsISWlJP5f03WT9NEmrJW2UdGfyONKy+uL31vPg01uH1rNA04yyv62Z2ZRQyRbEHwPrC9b/EvhyRLwa6AWuLuebd3b3ctNPuo4o7z3QX863NTObMioSEJIWAL8F3JKsC3gLcHeyy63ApeWsQ3vXTgoGMQGQTuGRTGZmiUq1IL4CfJJcrw7AHGB3RGSS9R5gfrEXSlohqUNSx/bt2ydcgbaWOdQdcfaa8PHMzI41kx4Qki4BtkVE50ReHxE3RURrRLQ2NzdPuB7LFjdx5qknHFY2mA1fpDYzS1SiBXE+8A5JzwF3kOta+jtgtqT81B8LgM3lrsh737TosPX6tG+WMzPLm/SAiIhPR8SCiFgCXA78e0RcCawCLkt2uwq4v9x1OeOUWdSnc91KAq4+/zTPxWRmlqim+yA+BXxc0kZy1yS+Ue43bO/aSWYwd6U6gFt++kvfKGdmlqjobK4R8UPgh8lyF3DOZL5/W8scUikNzcmUjdw1CLcizMyqqwVRGXForKs8YZ+Z2ZCaDoh71vQwWHAvxGA22PDivspVyMysitR0QOzY13dE2ffXbqlATczMqk9NB0TzrMYjys6cd3wFamJmVn1qOiCG3ygH0LVjfwVqYmZWfWo6IIpNzPfIL7Z5qKuZGTUeEG0tc2hIHz7/UoSn2zAzgxoPiGWLm1i54jzetKQJKXc3dUNdykNdzcyo8YDIe2rzHghIp8R1l5zpG+XMzHBA0N61k/5MlgAy2WDVhm2VrpKZWVWo+YBoa5lDquAyxENPb+X21ZsqVyEzsypR8wFR7LkQdz7mgDAzq/mAADhv2EXpp7fs9VBXM6t5NR8Qnd29fPM/fnlY2cBgcM+angrVyMysOtR8QLR37WSgcMa+xF0dz7sVYWY1reYDYvhF6ryBQd8wZ2a1reYDYtniJk5rnnlEuYRvmDOzmlbzAQHQMve4I8oWnzijAjUxM6sekx4QkhZKWiXpaUnrJP1xUn6ipIckPZN8n7TbmS8446Qjyp7beYArbm73dQgzq1mVaEFkgE9ExOuANuAjkl4HXAs8EhGnA48k65Oi2KyuAAOZLPes6eHGVRsdFGZWc+om+w0jYguwJVneJ2k9MB9YDlyQ7HYr8EPgU5NRp6YZDSNuu6vjeQazQUNdituuafM8TWZWMyp6DULSEuCNwGrg5CQ8AF4ETh7hNSskdUjq2L59+1Gpx7oX9hQtD3KjmbKRa014VJOZ1ZKKBYSkmcA9wMciYm/htogIcr+fjxARN0VEa0S0Njc3H5W6FH2jwroC9ck04J3dve5yMrOaMOldTACS6smFw20RcW9SvFXSvIjYImkeMGnTqr5r6QLufHQTRe6XI52Cy9+0iHcuXQDAlbe005/JusvJzI55lRjFJOAbwPqI+NuCTQ8AVyXLVwH3T1adli1u4vJzFhXd9pbXnMyf/87rWba4aWhq8MIuJ7cozOxYVYkWxPnA+4GnJD2elH0G+CLwHUlXA93AeyazUu9cuoDbV286ortp1YZt3L56E70H+mma0UBDXYqBTJb6uhRNMxrcojCzY1YlRjH9lFy3fjEXTmZdCi1b3MR/f3MLX/9x12HlmcHgM/c9BUBdCm5Y/nrWvrAHkbu4PbxFAbn5ndpa5jgszGxKq8g1iGq1ry9TcnsmC/f9vIenNueCoS4l6tIpBgfdojCzY48DosAzW/eNus/azXt5eSAL5IbAXnHuQubPnk5by5wjrlHcs6bHrQkzm7IcEInO7t4xXWg+ODA4tBzAgb4Mjz+/mwfXvch5LXOGrlGkU+Luzh4yg25NmNnU5IBItHftJDvaDRFF/OvjLwwtP9Gzh99/cwt7+zKs27yHpzbvOez6hAPCzKYSB0SirWUO9WnRX+xmiHF4eP1WenYfpG8gSwApQTolXth9kNtXbxq6wP3OpQscGGZW1RwQiWWLm3h360JuW73pFR1n14GBoXAQ8Pr5J7D+xX1HDKG9q7OHlR9uAzzqycyqkwOiwDuXLuDOx54nM5G+psSu/Ydmhg2gsS5FZjB7xP0VA5ksX//Rs6z6xTaycWgyQHBgmFl1cEAUWLa4iRuWn8Vn73tq1PmZxqqzu5dUSmSHdV1J8Mj6rUPXPfqTUU/3ruk5bJgsODDMrDIcEMO879xFnHHKLP5o5Ro27375FR9vMPL/yVl84gx6dh9kcFgrJSWxY1/fUPfUwAiBMZ6Q6OzudbiY2YQ5IIpYtriJr16xlPf+43+SyR7dY3fvOlC0fP7s6Ty8futQyyWAHfv6hu6r6BvIBQYc3qIYKQQ6u3t9056ZvSLKzaw9NbW2tkZHR0fZjn/76k187r6nOMoZMWH5+UmC3CyLrUuaeLxnz9B9F5ecPY+d+/u5+Kx5rHthz9AFdwFXnLtoaDk/M+3woLlnTc+I24cbHkzjXTezypHUGRGto+7ngBjZjas28qUHN0zo/ohqlkqSJn9erz1lFs9sf4lM0hWWTkE6laI/kyUlWPHfWvjNM08ZCpBZjXXc8tNfDl1c/8B5S4bW61LiDQtn07lpN1Fke0NdiusuOfOI4b7jCZDJDBsHnx2LHBBHQb6bZiCTReKodzfVCjHyQ5nSKfHbZ8/ju09uYTAbpFPihuVnccYps4YC6cxTT6D3QD9tLXN4aN2L3PSTLiKgsT4XPuu27OXis+bxvnMXcfvqTXx/7Zah9WK/0Eu1lAr33/DiPq67f23RIMwH3Q3fXVdyUEG1Bt9oHITHNgfEUZL/h/HC7oOsfHTTMdeaOJbMqE9xYOBQip84o57dBweGPrPTm4+ja8f+wx4MlQ+vdAqWLWqio7t3TJ+xgFedNJNnt700dM/La06ZxYat+8gG1CVdft99csthw5jzv1yH/8K9ffWmw8LolQ5IKLUOcM+aHnbs6wOgeVbjYSEMhz8Yq1gw5vd1WORMtQB1QBxlha2J+roUZ88/gUef80OCbOwE1KXFtLoU+/oOzek1qzF92Hq+bCAbZAaDs049nubjp/HzTb30ZbK89pRZnDp7Oo+s38pgQPPMRp7vPTAUbDMb0rzUf+h4aVH0aYkjaRhlRgEJIqA+Lb7wjrNYtWEb2/a+zHvftOiwll++hTba+tG61jXaew1vQeZDsWlGw2Hfx/tLfioOCHFAlMHwH8Yrbj7U/eSWhdkrlxakkotk2QgGS3TrjhZkwzXWib7M2PavT4uUxPT6FCEYyAQNaTF31jQ+dP5pANz52CZOPn4azbMah3oXUoLLz1nE9n19bNv7Mue1zGFvX4aNW/fRl8kOhejXf/TsUKgO7xrNh+yOfX3sPtDP5t0HQaJOoi8zSFvLHGY01r2iKXscEJNgeH91/gfmgjNO4vMPrKV/MA4beWRmdjQ11KVY+eHxt1jGGhBVdx+EpIuAvwPSwC0R8cUKV2lEyxY3DX0wyxY38b5zDz3X+oxTZh3W2mjv2knTjAbWvrCHuzqeZ2AwSAkWNs1gU+8BInJdEDMb0+zvG6yaobVmVr36yzxTdFUFhKQ0cCPwm0AP8JikByLi6crWbPwKwyO/nveupQtGHSGS7yvdsa+P5lmNzGqsY92WvZw57/jDmqzDm7AnTK9n7eY9HOwf5GDBsKu6lEdhmR2L9h0cKNuxqyoggHOAjRHRBSDpDmA5MOUCopRi4TH8L4BiZeM1liGeY1nPj3bZfaCfXfv7aWmeyQVnnMSqDdt4+oU99CUdxY11aeok9r08wKxp9dSnxUsvZ9jz8gAnTK9nZmMdu/b3s+fgANMb0ixsmsELew5ysD/LzMb0UL0PDgxywvR60inxUn+GgUxwXEOaGQ11vLjnIP2DcViXncj1Gaelw0LRrBas27K3bMeutoCYDzxfsN4DnFu4g6QVwAqARYsWYSMbaxCNZ71QYZfaZBttWOFYR7/ku/0KW2SzptePGqr50BzJ87sOsHXvy8yaVk8mm2V6Qx1nnXo8v9yxn8a6FLNnNAwF7onHNQAMLe97OUP3zv0gcXxjHfv7M0TAQDISovDCaQpQSpw4o4HdB/vZ3zdIQ1r0DWYZzJJshzqJTHKfSVpiIJu76IpEZIP+JOTzQ3ZTKTF/9nRe7h9k+0t9pFNCEtkIBgouDKeTEU351zmeJ9/FZ80r27Gr6iK1pMuAiyLimmT9/cC5EfHRYvtX+iK1WS2q5Jj/4TdCFtZprMNXh/9xkH/N2hf2DI0c6stkOW3ucUNT1xSONBLw80299A8G0+tTHDetnjqJXftzQZpf377vZQYDjp9Wx9yZjWx/qY8D/YOcOKOeuTMbeXrL3uTekjTT6lL0D2apS4m9BzOkU+TGEwNzjmvg4MAg+/sGmTUtzfHTGti1v48Tptfzh79++oT+UJuSo5gknQd8PiLelqx/GiAi/qLY/g4IM7PxG2tApCajMuPwGHC6pNMkNQCXAw9UuE5mZjWpqq5BRERG0keBH5Ab5vrNiFhX4WqZmdWkqgoIgIj4HvC9StfDzKzWVVsXk5mZVQkHhJmZFeWAMDOzoqpqmOt4SdoOdE/w5XOBHUexOpXkc6lOx8q5HCvnAT6XvMUR0TzaTlM6IF4JSR1jGQc8FfhcqtOxci7HynmAz2W83MVkZmZFOSDMzKyoWg6ImypdgaPI51KdjpVzOVbOA3wu41Kz1yDMzKy0Wm5BmJlZCQ4IMzMrqiYDQtJFkjZI2ijp2krXZywkPSfpKUmPS+pIyk6U9JCkZ5LvTUm5JH01Ob8nJS2tYL2/KWmbpLUFZeOut6Srkv2fkXRVFZ3L5yVtTj6XxyW9vWDbp5Nz2SDpbQXlFf/5k7RQ0ipJT0taJ+mPk/Ip9dmUOI8p97lImibpUUlPJOfyhaT8NEmrk3rdmcx0jaTGZH1jsn3JaOc4bhFRU1/kZol9FmgBGoAngNdVul5jqPdzwNxhZX8FXJssXwv8ZbL8duD75B7y1QasrmC93wwsBdZOtN7AiUBX8r0pWW6qknP5PPCnRfZ9XfKz1QiclvzMpavl5w+YByxNlmcB/5XUeUp9NiXOY8p9Lsn/25nJcj2wOvl//R3g8qT868AfJMt/CHw9Wb4cuLPUOU6kTrXYghh67nVE9AP5515PRcuBW5PlW4FLC8q/HTntwGxJ5XsuYQkR8WNg17Di8db7bcBDEbErInqBh4CLyl/7w41wLiNZDtwREX0R8UtgI7mfvar4+YuILRETfsENAAAEO0lEQVSxJlneB6wn98jfKfXZlDiPkVTt55L8v30pWa1PvgJ4C3B3Uj78M8l/VncDF0oSI5/juNViQBR77nWpH6hqEcCDkjqVey43wMkRsSVZfhE4OVmu9nMcb72r/Xw+mnS7fDPfJcMUOpeka+KN5P5inbKfzbDzgCn4uUhKS3oc2EYubJ8FdkdEpki9huqcbN8DzOEonkstBsRU9WsRsRS4GPiIpDcXboxc23LKjVmeqvUu8DXgVcAbgC3AlypbnfGRNBO4B/hYROwt3DaVPpsi5zElP5eIGIyINwALyP3V/5pK1qcWA2IzsLBgfUFSVtUiYnPyfRtwH7kfnq35rqPk+7Zk92o/x/HWu2rPJyK2Jv+os8DNHGrKV/25SKon90v1toi4Nymecp9NsfOYyp8LQETsBlYB55Hrzss/3K2wXkN1TrafAOzkKJ5LLQbElHvutaTjJM3KLwNvBdaSq3d+1MhVwP3J8gPA7yUjT9qAPQXdBtVgvPX+AfBWSU1JV8Fbk7KKG3Zt53fIfS6QO5fLk5EmpwGnA49SJT9/SV/1N4D1EfG3BZum1Gcz0nlMxc9FUrOk2cnydOA3yV1TWQVcluw2/DPJf1aXAf+etPpGOsfxm8yr9NXyRW5Exn+R69/7bKXrM4b6tpAblfAEsC5fZ3L9jY8AzwAPAyfGodEQNybn9xTQWsG6ryTXxB8g1xd69UTqDXyI3MW2jcAHq+hc/jmp65PJP8x5Bft/NjmXDcDF1fTzB/waue6jJ4HHk6+3T7XPpsR5TLnPBTgb+HlS57XAdUl5C7lf8BuBu4DGpHxasr4x2d4y2jmO98tTbZiZWVG12MVkZmZj4IAwM7OiHBBmZlaUA8LMzIpyQJiZWVEOCLMKkXSBpO9Wuh5mI3FAmJlZUQ4Is1FI+t1knv7HJf1jMqHaS5K+nMzb/4ik5mTfN0hqTyaJu0+HnqfwakkPJ3P9r5H0quTwMyXdLekXkm5L7gw2qwoOCLMSJL0WeC9wfuQmURsErgSOAzoi4kzgR8D1yUu+DXwqIs4mdydvvvw24MaI+BXgV8ndkQ252Uc/Rm4O/xbg/LKflNkY1Y2+i1lNuxBYBjyW/HE/ndwEdlngzmSffwHulXQCMDsifpSU3wrclcyjNT8i7gOIiJcBkuM9GhE9yfrjwBLgp+U/LbPROSDMShNwa0R8+rBC6X8N22+ic9b0FSwP4n+TVkXcxWRW2iPAZZJOgqFnNi8m928nP8Pm+4CfRsQeoFfSf0vK3w/8KHJPOuuRdGlyjEZJMyb1LMwmwH+tmJUQEU9L+hy5p/mlyM3k+hFgP3BOsm0buesUkJt++etJAHQBH0zK3w/8o6QbkmO8exJPw2xCPJur2QRIeikiZla6Hmbl5C4mMzMryi0IMzMryi0IMzMrygFhZmZFOSDMzKwoB4SZmRXlgDAzs6L+Pxo1UlBjxNORAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}